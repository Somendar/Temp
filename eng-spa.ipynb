{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Machine Translation Project (English to Spanish)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T08:25:30.790104Z","iopub.status.busy":"2024-06-05T08:25:30.789243Z","iopub.status.idle":"2024-06-05T08:25:42.976071Z","shell.execute_reply":"2024-06-05T08:25:42.975097Z","shell.execute_reply.started":"2024-06-05T08:25:30.790070Z"},"trusted":true},"outputs":[],"source":["import pathlib\n","import random\n","import string\n","import tensorflow.strings as tf_strings\n","import tensorflow.data as tf_data\n","import re\n","from keras.layers import TextVectorization\n","import keras\n","import tensorflow as tf\n","from keras import layers\n","import json"]},{"cell_type":"markdown","metadata":{},"source":["### Verify access to the GPU"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T08:25:42.978737Z","iopub.status.busy":"2024-06-05T08:25:42.977950Z","iopub.status.idle":"2024-06-05T08:25:43.381801Z","shell.execute_reply":"2024-06-05T08:25:43.380537Z","shell.execute_reply.started":"2024-06-05T08:25:42.978701Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[name: \"/device:CPU:0\"\n","device_type: \"CPU\"\n","memory_limit: 268435456\n","locality {\n","}\n","incarnation: 1754750633077784423\n","xla_global_id: -1\n","]\n"]}],"source":["from tensorflow.python.client import device_lib\n","print(device_lib.list_local_devices())"]},{"cell_type":"markdown","metadata":{},"source":["### Download and prepare the data\n","source :\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\""]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T08:25:43.384085Z","iopub.status.busy":"2024-06-05T08:25:43.383747Z","iopub.status.idle":"2024-06-05T08:25:43.776961Z","shell.execute_reply":"2024-06-05T08:25:43.775923Z","shell.execute_reply.started":"2024-06-05T08:25:43.384052Z"},"trusted":true},"outputs":[{"ename":"UnicodeDecodeError","evalue":"'charmap' codec can't decode byte 0x81 in position 1672: character maps to <undefined>","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)","Cell \u001b[1;32mIn[12], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m text_file \u001b[38;5;241m=\u001b[39m pathlib\u001b[38;5;241m.\u001b[39mPath(text_file)\u001b[38;5;241m.\u001b[39mparent \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspa-eng\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspa.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(text_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m---> 10\u001b[0m     lines \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     13\u001b[0m text_pairs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n","File \u001b[1;32m~\\Anaconda3\\lib\\encodings\\cp1252.py:23\u001b[0m, in \u001b[0;36mIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcodecs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcharmap_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdecoding_table\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n","\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x81 in position 1672: character maps to <undefined>"]}],"source":["text_file =  keras.utils.get_file(\n","    fname = \"spa-eng.zip\",\n","    origin = \"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n","    extract = True,\n",")\n","\n","text_file = pathlib.Path(text_file).parent / \"spa-eng\" / \"spa.txt\"\n","\n","with open(text_file, \"r\") as f:\n","    lines = f.read().split(\"\\n\")[:-1]\n","    \n","    \n","text_pairs = []\n","\n","for line in lines:\n","    eng, spa = line.split(\"\\t\")\n","    spa = \"[start] \" + spa + \" [end]\"\n","    text_pairs.append((eng, spa))"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T08:25:43.779467Z","iopub.status.busy":"2024-06-05T08:25:43.779155Z","iopub.status.idle":"2024-06-05T08:25:43.886497Z","shell.execute_reply":"2024-06-05T08:25:43.885677Z","shell.execute_reply.started":"2024-06-05T08:25:43.779443Z"},"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'text_pairs' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m random\u001b[38;5;241m.\u001b[39mshuffle(\u001b[43mtext_pairs\u001b[49m)\n","\u001b[1;31mNameError\u001b[0m: name 'text_pairs' is not defined"]}],"source":["random.shuffle(text_pairs)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T08:25:43.887842Z","iopub.status.busy":"2024-06-05T08:25:43.887545Z","iopub.status.idle":"2024-06-05T08:25:43.893298Z","shell.execute_reply":"2024-06-05T08:25:43.892413Z","shell.execute_reply.started":"2024-06-05T08:25:43.887818Z"},"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'text_pairs' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mtext_pairs\u001b[49m[i])\n","\u001b[1;31mNameError\u001b[0m: name 'text_pairs' is not defined"]}],"source":["for i in range(5):\n","    print(text_pairs[i])"]},{"cell_type":"markdown","metadata":{},"source":["Structure of the Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T08:25:43.895102Z","iopub.status.busy":"2024-06-05T08:25:43.894747Z","iopub.status.idle":"2024-06-05T08:25:43.905307Z","shell.execute_reply":"2024-06-05T08:25:43.904247Z","shell.execute_reply.started":"2024-06-05T08:25:43.895068Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["118964 total pairs\n","83276 training pairs\n","17844 validation pairs\n","17844 test pairs\n"]}],"source":["num_val_samples = int(0.15 * len(text_pairs))\n","num_train_samples = len(text_pairs) - 2 * num_val_samples\n","train_pairs = text_pairs[:num_train_samples]\n","val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n","test_pairs = text_pairs[num_train_samples + num_val_samples:]\n","\n","print(f\"{len(text_pairs)} total pairs\")\n","print(f\"{len(train_pairs)} training pairs\")\n","print(f\"{len(val_pairs)} validation pairs\")\n","print(f\"{len(test_pairs)} test pairs\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T08:25:43.906823Z","iopub.status.busy":"2024-06-05T08:25:43.906531Z","iopub.status.idle":"2024-06-05T08:25:43.914292Z","shell.execute_reply":"2024-06-05T08:25:43.913402Z","shell.execute_reply.started":"2024-06-05T08:25:43.906791Z"},"trusted":true},"outputs":[],"source":["# parameters\n","strip_chars = string.punctuation + \"¿\"\n","strip_chars = strip_chars.replace(\"[\", \"\")\n","strip_chars = strip_chars.replace(\"]\", \"\")\n","\n","vocab_size = 15000\n","sequence_length = 20\n","batch_size = 64"]},{"cell_type":"markdown","metadata":{},"source":["## Vectorize the data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T08:25:43.916003Z","iopub.status.busy":"2024-06-05T08:25:43.915720Z","iopub.status.idle":"2024-06-05T08:25:46.375409Z","shell.execute_reply":"2024-06-05T08:25:46.374618Z","shell.execute_reply.started":"2024-06-05T08:25:43.915980Z"},"trusted":true},"outputs":[],"source":["def custom_standardization(input_string):\n","    lowercase = tf_strings.lower(input_string)\n","    return tf_strings.regex_replace(lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n","\n","# vectorization\n","eng_vectorization = TextVectorization(\n","    max_tokens = vocab_size,\n","    output_mode = \"int\",\n","    output_sequence_length = sequence_length,\n",")\n","\n","spa_vectorization = TextVectorization(\n","    max_tokens = vocab_size,\n","    output_mode = \"int\",\n","    output_sequence_length = sequence_length + 1,\n","    standardize = custom_standardization,\n",")\n","\n","train_eng_texts = [pair[0] for pair in train_pairs]\n","train_spa_texts = [pair[1] for pair in train_pairs]\n","\n","eng_vectorization.adapt(train_eng_texts)\n","spa_vectorization.adapt(train_spa_texts)\n","\n","#save the vectorization layers\n","eng_vectorization_config = eng_vectorization.get_config()\n","eng_vectorization_config.pop('standardize', None)\n","eng_vocab = eng_vectorization.get_vocabulary()\n","with open('eng_vectorization_config.json', 'w', encoding='utf-8') as f:\n","    json.dump(eng_vectorization_config, f)\n","    \n","with open('eng_vocab.json', 'w', encoding='utf-8') as f:\n","    json.dump(eng_vocab, f)\n","    \n","spa_vectorization_config = spa_vectorization.get_config()\n","spa_vectorization_config.pop('standardize', None)\n","spa_vocab = spa_vectorization.get_vocabulary()\n","with open('spa_vectorization_config.json', 'w', encoding='utf-8') as f:\n","    json.dump(spa_vectorization_config, f)\n","    \n","with open('spa_vocab.json', 'w', encoding='utf-8') as f:\n","    json.dump(spa_vocab, f)\n","    \n","\n","def format_dataset(eng, spa):\n","    eng = eng_vectorization(eng)\n","    spa = spa_vectorization(spa)\n","    return (\n","        {\n","            \"encoder_inputs\": eng,\n","            \"decoder_inputs\": spa[:, :-1],\n","        },\n","        spa[:, 1:],\n","    )\n","    \n","def make_dataset(pairs):\n","    eng_texts, spa_texts = zip(*pairs)\n","    eng_texts = list(eng_texts)\n","    spa_texts = list(spa_texts)\n","    dataset = tf_data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n","    dataset = dataset.batch(batch_size)\n","    dataset = dataset.map(format_dataset)\n","    return dataset.cache().shuffle(2048).prefetch(16)\n","\n","train_ds = make_dataset(train_pairs)\n","val_ds = make_dataset(val_pairs)\n","    "]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T08:25:46.376913Z","iopub.status.busy":"2024-06-05T08:25:46.376584Z","iopub.status.idle":"2024-06-05T08:25:47.152265Z","shell.execute_reply":"2024-06-05T08:25:47.151246Z","shell.execute_reply.started":"2024-06-05T08:25:46.376883Z"},"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'train_ds' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs,targets \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtrain_ds\u001b[49m\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_inputs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(targets\u001b[38;5;241m.\u001b[39mshape)\n","\u001b[1;31mNameError\u001b[0m: name 'train_ds' is not defined"]}],"source":["for inputs,targets in train_ds.take(1):\n","    print(inputs[\"encoder_inputs\"].shape)\n","    print(targets.shape)"]},{"cell_type":"markdown","metadata":{},"source":["### Model Architecture\n","![Encoder-Decoder](images/encoder-decoder-context.png)\n","![Encoder-Decoder](images/encoder-decoder-translation.png)\n","![Attention Mechanism](images/attention.png)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T08:25:47.155519Z","iopub.status.busy":"2024-06-05T08:25:47.155203Z","iopub.status.idle":"2024-06-05T08:25:47.182230Z","shell.execute_reply":"2024-06-05T08:25:47.181220Z","shell.execute_reply.started":"2024-06-05T08:25:47.155494Z"},"trusted":true},"outputs":[],"source":["# Creating an Encoder\n","class TransformerEncoder(layers.Layer):\n","    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n","        super().__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.dense_dim = dense_dim\n","        self.num_heads = num_heads\n","        self.attention = layers.MultiHeadAttention(\n","            num_heads = num_heads, key_dim = embed_dim\n","        )\n","        self.dense_proj = keras.Sequential(\n","            [\n","                layers.Dense(dense_dim, activation = \"relu\"),\n","                layers.Dense(embed_dim),\n","            ]\n","        )\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","        self.supports_masking = True\n","        \n","    def call(self, inputs, mask=None):\n","        if mask is not None:\n","            padding_mask = tf.cast(mask[:, None, :], dtype = tf.int32)\n","        else:\n","            padding_mask = None\n","            \n","        attention_output = self.attention(\n","            query = inputs,\n","            value = inputs,\n","            key = inputs,\n","            attention_mask = padding_mask,\n","        )\n","        proj_input = self.layernorm_1(inputs + attention_output)\n","        proj_output = self.dense_proj(proj_input)\n","        return self.layernorm_2(proj_input + proj_output)\n","    \n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({\n","            \"embed_dim\": self.embed_dim,\n","            \"dense_dim\": self.dense_dim,\n","            \"num_heads\": self.num_heads,\n","        })\n","        return config\n","    \n","# Creating a Positional Embedding\n","class PositionalEmbedding(layers.Layer):\n","    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n","        super().__init__(**kwargs)\n","        self.token_embeddings = layers.Embedding(\n","            input_dim = vocab_size, output_dim = embed_dim\n","        )\n","        self.position_embeddings = layers.Embedding(\n","            input_dim = sequence_length, output_dim = embed_dim\n","        )\n","        self.sequence_length = sequence_length\n","        self.vocab_size = vocab_size\n","        self.embed_dim = embed_dim\n","        \n","    def call(self, inputs):\n","        length = tf.shape(inputs)[-1]\n","        positions = tf.range(start = 0, limit = length, delta = 1)\n","        embedded_tokens = self.token_embeddings(inputs)\n","        embedded_positions = self.position_embeddings(positions)\n","        return embedded_tokens + embedded_positions\n","    \n","    def compute_mask(self, inputs, mask=None):\n","        if mask is not None:\n","            return tf.not_equal(inputs, 0)\n","        else:\n","            return None\n","        \n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({\n","            \"vocab_size\": self.vocab_size,\n","            \"sequence_length\": self.sequence_length,\n","            \"embed_dim\": self.embed_dim,\n","        })\n","        return config\n","    \n","# Creating a Decoder\n","class TransformerDecoder(layers.Layer):\n","    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n","        super().__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.latent_dim = latent_dim\n","        self.num_heads = num_heads\n","        self.attention_1 = layers.MultiHeadAttention(\n","            num_heads = num_heads, key_dim = embed_dim\n","        )\n","        self.attention_2 = layers.MultiHeadAttention(\n","            num_heads = num_heads, key_dim = embed_dim\n","        )\n","        self.dense_proj = keras.Sequential(\n","            [\n","                layers.Dense(latent_dim, activation = \"relu\"),\n","                layers.Dense(embed_dim),\n","            ]\n","        )\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","        self.layernorm_3 = layers.LayerNormalization()\n","        self.supports_masking = True\n","        \n","    def call(self, inputs, encoder_outputs, mask=None):\n","        casual_mask = self.get_causal_attention_mask(inputs)\n","        if mask is not None:\n","            padding_mask = tf.cast(mask[:, None, :], dtype = tf.int32)\n","            padding_mask = tf.minimum(padding_mask, casual_mask)\n","        else:\n","            padding_mask = None\n","            \n","        attention_output_1 = self.attention_1(\n","            query = inputs,\n","            value = inputs,\n","            key = inputs,\n","            attention_mask = casual_mask,\n","        )\n","        out_1 = self.layernorm_1(inputs + attention_output_1)\n","        \n","        attention_output_2 = self.attention_2(\n","            query = out_1,\n","            value = encoder_outputs,\n","            key = encoder_outputs,\n","            attention_mask = padding_mask,\n","        )\n","        \n","        out_2 = self.layernorm_2(out_1 + attention_output_2)\n","        proj_output = self.dense_proj(out_2)\n","        \n","        return self.layernorm_3(out_2 + proj_output)\n","    \n","    def get_causal_attention_mask(self, inputs):\n","        input_shape = tf.shape(inputs)\n","        batch_size, sequence_length = input_shape[0], input_shape[1]\n","        i = tf.range(sequence_length)[:, None]\n","        j = tf.range(sequence_length)\n","        mask = tf.cast(i >= j, tf.int32)\n","        mask = tf.reshape(mask,(1, input_shape[1], input_shape[1]))\n","        mult = tf.concat(\n","            [\n","                tf.expand_dims(batch_size, -1),\n","                tf.convert_to_tensor([1, 1]),\n","            ],\n","            axis = 0,\n","        )\n","        return tf.tile(mask, mult)\n","    \n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({\n","            \"embed_dim\": self.embed_dim,\n","            \"latent_dim\": self.latent_dim,\n","            \"num_heads\": self.num_heads,\n","        })\n","        return config\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T08:25:47.184280Z","iopub.status.busy":"2024-06-05T08:25:47.183484Z","iopub.status.idle":"2024-06-05T08:25:47.913298Z","shell.execute_reply":"2024-06-05T08:25:47.912498Z","shell.execute_reply.started":"2024-06-05T08:25:47.184245Z"},"trusted":true},"outputs":[],"source":["# define emmbedding dimensions, latent dimensions, and number of heads\n","embed_dim = 256\n","latent_dim = 2048\n","num_heads = 8\n","\n","#Encoder\n","encoder_inputs = keras.Input(shape = (None,), dtype = \"int64\", name = \"encoder_inputs\")\n","\n","x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n","\n","encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n","\n","encoder = keras.Model(encoder_inputs, encoder_outputs, name = \"encoder\")\n","\n","#Decoder\n","decoder_inputs = keras.Input(shape = (None,), dtype = \"int64\", name = \"decoder_inputs\")\n","encoder_seq_inputs = keras.Input(shape = (None, embed_dim), name = \"encoder_seq_inputs\")\n","\n","x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n","\n","x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoder_seq_inputs)\n","\n","x = layers.Dropout(0.5)(x)\n","\n","decoder_outputs = layers.Dense(vocab_size, activation = \"softmax\")(x)\n","\n","decoder = keras.Model([decoder_inputs, encoder_seq_inputs], decoder_outputs, name = \"decoder\")\n","\n","# Define the final model\n","decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n","\n","transformer = keras.Model(\n","    [encoder_inputs, decoder_inputs], decoder_outputs, name = \"transformer\"\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-05T08:25:47.914695Z","iopub.status.busy":"2024-06-05T08:25:47.914403Z"},"trusted":true},"outputs":[{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"transformer\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mModel: \"transformer\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n","│ encoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ positional_embeddi… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,845,120</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEmbeddi…</span> │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ decoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ transformer_encoder │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,155,456</span> │ positional_embed… │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncode…</span> │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ decoder             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">12,959,640</span> │ decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">15000</span>)            │            │ transformer_enco… │\n","└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n","│ encoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n","│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ positional_embeddi… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │  \u001b[38;5;34m3,845,120\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n","│ (\u001b[38;5;33mPositionalEmbeddi…\u001b[0m │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ decoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n","│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ transformer_encoder │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │  \u001b[38;5;34m3,155,456\u001b[0m │ positional_embed… │\n","│ (\u001b[38;5;33mTransformerEncode…\u001b[0m │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ decoder             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      │ \u001b[38;5;34m12,959,640\u001b[0m │ decoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n","│ (\u001b[38;5;33mFunctional\u001b[0m)        │ \u001b[38;5;34m15000\u001b[0m)            │            │ transformer_enco… │\n","└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,960,216</span> (76.14 MB)\n","</pre>\n"],"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m19,960,216\u001b[0m (76.14 MB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,960,216</span> (76.14 MB)\n","</pre>\n"],"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m19,960,216\u001b[0m (76.14 MB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"],"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","\u001b[1m   2/1302\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:29\u001b[0m 69ms/step - accuracy: 0.1627 - loss: 9.1578      "]},{"name":"stderr","output_type":"stream","text":["WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","I0000 00:00:1717575963.476033      87 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","W0000 00:00:1717575963.507169      87 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","W0000 00:00:1717575963.542423      87 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","W0000 00:00:1717575963.555653      87 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m 735/1302\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 53ms/step - accuracy: 0.6986 - loss: 2.4653"]},{"name":"stderr","output_type":"stream","text":["W0000 00:00:1717576002.555536      85 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","W0000 00:00:1717576002.566677      85 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m1301/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.7124 - loss: 2.2065"]},{"name":"stderr","output_type":"stream","text":["W0000 00:00:1717576027.633448      84 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","W0000 00:00:1717576030.765933      85 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 53ms/step - accuracy: 0.7124 - loss: 2.2058 - val_accuracy: 0.8050 - val_loss: 1.2168\n","Epoch 2/20\n","\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 47ms/step - accuracy: 0.8150 - loss: 1.1768 - val_accuracy: 0.8504 - val_loss: 0.9077\n","Epoch 3/20\n","\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 46ms/step - accuracy: 0.8541 - loss: 0.8872 - val_accuracy: 0.8672 - val_loss: 0.7758\n","Epoch 4/20\n","\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 47ms/step - accuracy: 0.8690 - loss: 0.7716 - val_accuracy: 0.8743 - val_loss: 0.7340\n","Epoch 5/20\n","\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 47ms/step - accuracy: 0.8782 - loss: 0.7020 - val_accuracy: 0.8791 - val_loss: 0.7150\n","Epoch 6/20\n","\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 47ms/step - accuracy: 0.8864 - loss: 0.6460 - val_accuracy: 0.8806 - val_loss: 0.6958\n","Epoch 7/20\n","\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 47ms/step - accuracy: 0.8916 - loss: 0.6129 - val_accuracy: 0.8822 - val_loss: 0.6948\n","Epoch 8/20\n","\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 47ms/step - accuracy: 0.8971 - loss: 0.5803 - val_accuracy: 0.8837 - val_loss: 0.6919\n","Epoch 9/20\n","\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 47ms/step - accuracy: 0.9002 - loss: 0.5607 - val_accuracy: 0.8847 - val_loss: 0.6905\n","Epoch 10/20\n","\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 47ms/step - accuracy: 0.9033 - loss: 0.5436 - val_accuracy: 0.8856 - val_loss: 0.6893\n","Epoch 11/20\n","\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 47ms/step - accuracy: 0.9075 - loss: 0.5232 - val_accuracy: 0.8854 - val_loss: 0.7062\n","Epoch 12/20\n","\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 47ms/step - accuracy: 0.9091 - loss: 0.5131 - val_accuracy: 0.8856 - val_loss: 0.7145\n","Epoch 13/20\n","\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 47ms/step - accuracy: 0.9124 - loss: 0.4964 - val_accuracy: 0.8876 - val_loss: 0.7122\n","Epoch 14/20\n","\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 47ms/step - accuracy: 0.9139 - loss: 0.4899 - val_accuracy: 0.8835 - val_loss: 0.7289\n","Epoch 15/20\n","\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 47ms/step - accuracy: 0.9154 - loss: 0.4808 - val_accuracy: 0.8864 - val_loss: 0.7295\n","Epoch 16/20\n","\u001b[1m 311/1302\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m43s\u001b[0m 44ms/step - accuracy: 0.9216 - loss: 0.4409"]}],"source":["epochs = 10\n","\n","transformer.summary()\n","\n","transformer.compile(\n","    \"rmsprop\", loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"]\n",")\n","\n","transformer.fit(train_ds, epochs = epochs, validation_data = val_ds)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["spa_vocab = spa_vectorization.get_vocabulary()\n","spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n","max_decoded_sentence_length = sequence_length\n","\n","def decode_sentence(input_sentence):\n","    tokenized_input_sentence = eng_vectorization([input_sentence])\n","    decoded_sentence = \"[start]\"\n","    for i in range(max_decoded_sentence_length):\n","        tokenized_target_sentence = spa_vectorization([decoded_sentence])[:, :-1]\n","        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n","        sampled_token_index = tf.argmax(predictions[0, i, :]).numpy().item(0)\n","        sampled_token = spa_index_lookup[sampled_token_index]\n","        decoded_sentence += \" \" + sampled_token\n","        if sampled_token == \"[end]\":\n","            break\n","    return decoded_sentence\n","\n","test_eng_texts = [pair[0] for pair in test_pairs]\n","for _ in range(5):\n","    input_sentence = random.choice(test_eng_texts)\n","    input_sentence = input_sentence.lower()\n","    input_sentence = input_sentence.translate(str.maketrans('', '', strip_chars))\n","    translated = decode_sentence(input_sentence)\n","    print(f\"input: {input_sentence}\")\n","    print(f\"translated: {translated}\")\n","    print()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["transformer.save(\"transformer.model.h5\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5147425,"sourceId":8602944,"sourceType":"datasetVersion"}],"dockerImageVersionId":30716,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":4}
